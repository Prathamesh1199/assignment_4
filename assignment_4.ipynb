{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "General Linear Model:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. What is the purpose of the General Linear Model (GLM)?\n\nThe purpose of the GLM is to model the relationship between a dependent variable and one or more independent variables. The GLM can be used to model a variety of different types of data, including continuous, binary, and categorical data.\n\n2. What are the key assumptions of the General Linear Model?\n\nThe key assumptions of the GLM are:\n\nThe dependent variable is normally distributed.\nThe independent variables are not correlated.\nThe errors are independent and identically distributed (i.i.d.).\nThe errors have a constant variance.\n3. How do you interpret the coefficients in a GLM?\n\nThe coefficients in a GLM can be interpreted as the average change in the dependent variable for a one-unit change in the independent variable. For example, if the coefficient for an independent variable is 1, then a one-unit increase in the independent variable is associated with a one-unit increase in the dependent variable.\n\n4. What is the difference between a univariate and multivariate GLM?\n\nA univariate GLM is a GLM with a single dependent variable. A multivariate GLM is a GLM with multiple dependent variables.\n\n5. Explain the concept of interaction effects in a GLM.\n\nAn interaction effect in a GLM occurs when the effect of one independent variable on the dependent variable depends on the value of another independent variable. For example, the effect of a drug on blood pressure may depend on the patient's age.\n\n6. How do you handle categorical predictors in a GLM?\n\nCategorical predictors in a GLM are typically handled by creating dummy variables. A dummy variable is a variable that takes on the value of 1 if the observation belongs to a particular category and 0 otherwise.\n\n7. What is the purpose of the design matrix in a GLM?\n\nThe design matrix in a GLM is a matrix that contains the values of the independent variables for each observation. The design matrix is used to calculate the coefficients in the GLM.\n\n8. How do you test the significance of predictors in a GLM?\n\nThe significance of predictors in a GLM can be tested using the t-test or the F-test. The t-test is used to test the significance of individual coefficients, while the F-test is used to test the significance of the overall model.\n\n9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n\nType I sums of squares are calculated by comparing the full model to a model with no predictors. Type II sums of squares are calculated by comparing the full model to a model with a single predictor removed. Type III sums of squares are calculated by comparing the full model to a model with all possible two-way interactions removed.\n\n10. Explain the concept of deviance in a GLM.\n\nThe deviance in a GLM is a measure of how well the model fits the data. The deviance is calculated as the difference between the observed and expected values of the dependent variable. A smaller deviance indicates a better fit to the data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Regression:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "11. What is regression analysis and what is its purpose?\n\nRegression analysis is a statistical method that is used to model the relationship between one or more independent variables and a dependent variable. The purpose of regression analysis is to predict the value of the dependent variable based on the values of the independent variables.\n\n12. What is the difference between simple linear regression and multiple linear regression?\n\nSimple linear regression is a type of regression analysis that uses one independent variable to predict the value of a dependent variable. Multiple linear regression is a type of regression analysis that uses multiple independent variables to predict the value of a dependent variable.\n\n13. How do you interpret the R-squared value in regression?\n\nThe R-squared value is a measure of how well the regression model fits the data. The R-squared value is calculated as the percentage of the variance in the dependent variable that is explained by the independent variables. An R-squared value of 1 indicates that the model perfectly fits the data, while an R-squared value of 0 indicates that the model does not fit the data at all.\n\n14. What is the difference between correlation and regression?\n\nCorrelation is a measure of the strength of the linear relationship between two variables. Regression is a statistical method that is used to model the relationship between one or more independent variables and a dependent variable. Correlation does not imply causation, while regression can be used to infer causation.\n\n15. What is the difference between the coefficients and the intercept in regression?\n\nThe coefficients in a regression model are the weights that are assigned to the independent variables. The intercept in a regression model is the value of the dependent variable when all of the independent variables are equal to 0.\n\n16. How do you handle outliers in regression analysis?\n\nOutliers are data points that are significantly different from the rest of the data. Outliers can be handled in a number of ways, including:\n\nIgnoring the outliers.\nImputing the outliers with the mean or median of the data.\nWinsorizing the outliers.\n17. What is the difference between ridge regression and ordinary least squares regression?\n\nRidge regression and ordinary least squares regression are both methods for fitting linear regression models. Ridge regression penalizes the coefficients in the model, which helps to reduce the variance of the estimates. Ordinary least squares regression does not penalize the coefficients, which can lead to overfitting.\n\n18. What is heteroscedasticity in regression and how does it affect the model?\n\nHeteroscedasticity is a condition in which the variance of the residuals in a regression model is not constant. Heteroscedasticity can affect the accuracy of the regression model.\n\n19. How do you handle multicollinearity in regression analysis?\n\nMulticollinearity is a condition in which two or more independent variables are highly correlated. Multicollinearity can affect the accuracy of the regression model.\n\n20. What is polynomial regression and when is it used?\n\nPolynomial regression is a type of regression analysis that uses polynomial functions to model the relationship between the independent and dependent variables. Polynomial regression is used when the relationship between the independent and dependent variables is not linear.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Loss function:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "21. What is a loss function and what is its purpose in machine learning?\n\nA loss function is a function that measures the error between the predicted values of a machine learning model and the actual values. The loss function is used to guide the learning process of the model, and to evaluate the performance of the model.\n\n22. What is the difference between a convex and non-convex loss function?\n\nA convex loss function is a loss function that has a bowl-shaped curve. This means that the loss function decreases monotonically as the model's predictions get closer to the actual values. A non-convex loss function is a loss function that does not have a bowl-shaped curve. This means that the loss function may have multiple local minima, which can make it difficult to find the global minimum.\n\n23. What is mean squared error (MSE) and how is it calculated?\n\nMean squared error (MSE) is a loss function that measures the squared difference between the predicted values and the actual values. MSE is calculated as the average of the squared errors for all of the data points.\n\n24. What is mean absolute error (MAE) and how is it calculated?\n\nMean absolute error (MAE) is a loss function that measures the absolute difference between the predicted values and the actual values. MAE is calculated as the average of the absolute errors for all of the data points.\n\n25. What is log loss (cross-entropy loss) and how is it calculated?\n\nLog loss (cross-entropy loss) is a loss function that is used for classification problems. Log loss measures the difference between the predicted probability distribution and the actual probability distribution. Log loss is calculated as the negative log likelihood of the actual values.\n\n26. How do you choose the appropriate loss function for a given problem?\n\nThe choice of loss function depends on the type of problem that you are trying to solve. For example, if you are trying to solve a regression problem, then you would typically use MSE or MAE. If you are trying to solve a classification problem, then you would typically use log loss.\n\n27. Explain the concept of regularization in the context of loss functions.\n\nRegularization is a technique that is used to prevent overfitting. Overfitting occurs when a model learns the training data too well, and as a result, it does not generalize well to new data. Regularization works by adding a penalty to the loss function that discourages the model from learning too complex of a function.\n\n28. What is Huber loss and how does it handle outliers?\n\nHuber loss is a loss function that is designed to handle outliers. Huber loss is a combination of MSE and MAE, and it is less sensitive to outliers than MSE.\n\n29. What is quantile loss and when is it used?\n\nQuantile loss is a loss function that is used to measure the error between the predicted values and the actual values at a specific quantile. Quantile loss is used when you want to measure the error at a specific point in the distribution, such as the median or the 95th percentile.\n\n30. What is the difference between squared loss and absolute loss?\n\nThe main difference between squared loss and absolute loss is that squared loss is more sensitive to outliers than absolute loss. This is because squared loss penalizes large errors more than absolute loss.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Optimizer (GD):",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "31. What is an optimizer and what is its purpose in machine learning?\n\nAn optimizer is an algorithm that is used to find the minimum of a function. In machine learning, optimizers are used to find the parameters of a model that minimize the loss function.\n\n32. What is Gradient Descent (GD) and how does it work?\n\nGradient descent is an optimization algorithm that works by iteratively moving in the direction of the negative gradient of the loss function. The gradient of the loss function is a vector that points in the direction of the steepest ascent of the loss function. By moving in the direction of the negative gradient, the optimizer is able to find the minimum of the loss function.\n\n33. What are the different variations of Gradient Descent?\n\nThere are many different variations of gradient descent. Some of the most common variations include:\n\nBatch gradient descent: This is the simplest variation of gradient descent. The optimizer uses the entire training dataset to calculate the gradient of the loss function.\nMini-batch gradient descent: This variation of gradient descent uses a small subset of the training dataset to calculate the gradient of the loss function. This can be more efficient than batch gradient descent, but it may not converge as quickly.\nStochastic gradient descent: This variation of gradient descent uses a single data point to calculate the gradient of the loss function. This can be the most efficient variation of gradient descent, but it may not converge as quickly.\n34. What is the learning rate in GD and how do you choose an appropriate value?\n\nThe learning rate is a hyperparameter that controls how much the optimizer moves in the direction of the negative gradient. A higher learning rate will cause the optimizer to move more quickly, while a lower learning rate will cause the optimizer to move more slowly. The appropriate value of the learning rate depends on the problem that you are trying to solve.\n\n35. How does GD handle local optima in optimization problems?\n\nGradient descent can be trapped in local optima. A local optimum is a point in the loss function that is not the global minimum, but it is the minimum in the vicinity of the point. Gradient descent can be trapped in a local optimum if the learning rate is too high.\n\n36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n\nStochastic gradient descent is a variation of gradient descent that uses a single data point to calculate the gradient of the loss function. This can be the most efficient variation of gradient descent, but it may not converge as quickly. SGD differs from GD in that it uses a single data point to calculate the gradient, while GD uses the entire training dataset.\n\n37. Explain the concept of batch size in GD and its impact on training.\n\nThe batch size is the number of data points that are used to calculate the gradient of the loss function. A larger batch size will cause the optimizer to move more smoothly, while a smaller batch size will cause the optimizer to be more responsive to changes in the loss function. The batch size can impact the training time and the accuracy of the model.\n\n38. What is the role of momentum in optimization algorithms?\n\nMomentum is a technique that is used to improve the convergence of gradient descent. Momentum works by storing a running average of the gradients and using that average to update the parameters of the model. This can help the optimizer to overcome local optima and converge to the global minimum more quickly.\n\n39. What is the difference between batch GD, mini-batch GD, and SGD?\n\nBatch GD uses the entire training dataset to calculate the gradient of the loss function. Mini-batch GD uses a small subset of the training dataset to calculate the gradient of the loss function. SGD uses a single data point to calculate the gradient of the loss function.\n\n40. How does the learning rate affect the convergence of GD?\n\nThe learning rate is a hyperparameter that controls how much the optimizer moves in the direction of the negative gradient. A higher learning rate will cause the optimizer to move more quickly, but it may not converge as quickly. A lower learning rate will cause the optimizer to move more slowly, but it may converge more quickly.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Regularization:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "41. What is regularization and why is it used in machine learning?\n\nRegularization is a technique that is used to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well, and as a result, it does not generalize well to new data. Regularization works by adding a penalty to the loss function that discourages the model from learning too complex of a function.\n\n42. What is the difference between L1 and L2 regularization?\n\nL1 and L2 regularization are two types of regularization that are used to prevent overfitting in machine learning models. L1 regularization adds a penalty to the loss function that is proportional to the absolute values of the coefficients. L2 regularization adds a penalty to the loss function that is proportional to the squared values of the coefficients.\n\n43. Explain the concept of ridge regression and its role in regularization.\n\nRidge regression is a type of linear regression that uses L2 regularization. Ridge regression adds a penalty to the loss function that is proportional to the squared values of the coefficients. This penalty discourages the model from learning coefficients that are too large.\n\n44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n\nElastic net regularization is a type of regularization that combines L1 and L2 regularization. Elastic net regularization uses a combination of the L1 and L2 penalties, which can help to improve the performance of the model.\n\n45. How does regularization help prevent overfitting in machine learning models?\n\nRegularization helps to prevent overfitting by discouraging the model from learning too complex of a function. This is because the penalty term in the loss function discourages the model from assigning large values to the coefficients. This can help to prevent the model from fitting the noise in the training data, and can improve the generalization performance of the model.\n\n46. What is early stopping and how does it relate to regularization?\n\nEarly stopping is a technique that is used to prevent overfitting in machine learning models. Early stopping works by stopping the training of the model early, before the model has had a chance to overfit the training data. Early stopping is related to regularization, because both techniques are used to prevent overfitting. However, early stopping is a more aggressive technique than regularization, and it can sometimes lead to a decrease in the performance of the model.\n\n47. Explain the concept of dropout regularization in neural networks.\n\nDropout regularization is a technique that is used to prevent overfitting in neural networks. Dropout regularization works by randomly dropping out nodes in the neural network during training. This means that the model is forced to learn to rely on the remaining nodes, which can help to prevent the model from overfitting the training data.\n\n48. How do you choose the regularization parameter in a model?\n\nThe regularization parameter is a hyperparameter that controls the amount of regularization that is applied to the model. The regularization parameter is typically chosen by cross-validation. Cross-validation involves training the model on a subset of the training data, and then evaluating the model on the remaining data. The regularization parameter that results in the best performance on the validation data is typically chosen.\n\n49. What is the difference between feature selection and regularization?\n\nFeature selection and regularization are two techniques that are used to improve the performance of machine learning models. Feature selection involves selecting a subset of the features that are most important for the model. Regularization involves adding a penalty to the loss function that discourages the model from learning too complex of a function.\n\n50. What is the trade-off between bias and variance in regularized models?\n\nBias is the difference between the expected value of the model's predictions and the true value of the target variable. Variance is the variability of the model's predictions around the expected value. Regularized models typically have lower variance than unregularized models, but they may also have higher bias. The trade-off between bias and variance is a fundamental tradeoff in machine learning, and it is important to consider both bias and variance when choosing a model.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "SVM:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "51. What is Support Vector Machines (SVM) and how does it work?\n\nSupport Vector Machines (SVM) are a type of supervised machine learning algorithm that can be used for both classification and regression tasks. SVM works by finding the hyperplane that best separates the two classes of data. The hyperplane is a line or a plane that divides the data into two regions, with each region containing only one class of data.\n\n52. How does the kernel trick work in SVM?\n\nThe kernel trick is a technique that is used to transform the data into a higher-dimensional space where the data is linearly separable. This allows SVM to be used for non-linear classification problems.\n\n53. What are support vectors in SVM and why are they important?\n\nSupport vectors are the data points that are closest to the hyperplane. These points are important because they determine the position of the hyperplane. The more support vectors there are, the more accurate the model will be.\n\n54. Explain the concept of the margin in SVM and its impact on model performance.\n\nThe margin is the distance between the hyperplane and the closest data points. A larger margin means that the model is more confident in its predictions. A smaller margin means that the model is less confident in its predictions.\n\n55. How do you handle unbalanced datasets in SVM?\n\nThere are a few ways to handle unbalanced datasets in SVM. One way is to use a cost-sensitive learning algorithm. This type of algorithm assigns different costs to misclassifying different classes of data. Another way to handle unbalanced datasets is to use a weighted SVM. This type of SVM assigns different weights to the different classes of data.\n\n56. What is the difference between linear SVM and non-linear SVM?\n\nLinear SVM can only be used for linearly separable problems. Non-linear SVM can be used for both linearly and non-linearly separable problems. Non-linear SVM uses the kernel trick to transform the data into a higher-dimensional space where the data is linearly separable.\n\n57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n\nThe C-parameter is a hyperparameter that controls the trade-off between the margin and the number of support vectors. A higher C-parameter means that the model will have a larger margin, but it will also have fewer support vectors. A lower C-parameter means that the model will have a smaller margin, but it will have more support vectors.\n\n58. Explain the concept of slack variables in SVM.\n\nSlack variables are used to allow some data points to be misclassified. This is done to prevent the model from becoming too complex and overfitting the data.\n\n59. What is the difference between hard margin and soft margin in SVM?\n\nHard margin SVM does not allow any data points to be misclassified. Soft margin SVM allows some data points to be misclassified. Hard margin SVM is more likely to overfit the data, while soft margin SVM is less likely to overfit the data.\n\n60. How do you interpret the coefficients in an SVM model?\n\nThe coefficients in an SVM model represent the importance of the different features. The larger the coefficient, the more important the feature. The coefficients can be used to interpret the model and to understand how the model makes predictions.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Decision Trees:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "61. What is a decision tree and how does it work?\n\nA decision tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. Decision trees work by recursively splitting the data into smaller and smaller subsets until the data is classified or a prediction is made.\n\n62. How do you make splits in a decision tree?\n\nSplits in a decision tree are made by choosing a feature and a threshold value for that feature. The data is then split into two subsets, one for the data points that are less than or equal to the threshold value and one for the data points that are greater than the threshold value.\n\n63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n\nImpurity measures are used to quantify the homogeneity of a dataset. The more homogeneous a dataset is, the lower its impurity. Impurity measures are used in decision trees to choose the best feature to split the data on.\n\n64. Explain the concept of information gain in decision trees.\n\nInformation gain is a measure of how much information is gained by splitting the data on a particular feature. The higher the information gain, the more informative the feature is. Information gain is used in decision trees to choose the best feature to split the data on.\n\n65. How do you handle missing values in decision trees?\n\nThere are a few ways to handle missing values in decision trees. One way is to simply drop the data points with missing values. Another way is to impute the missing values with the mean or median of the feature.\n\n66. What is pruning in decision trees and why is it important?\n\nPruning is a technique that is used to reduce the complexity of a decision tree. Pruning can improve the accuracy of the model by preventing the model from overfitting the data.\n\n67. What is the difference between a classification tree and a regression tree?\n\nA classification tree is used to classify data into different categories. A regression tree is used to predict a continuous value.\n\n68. How do you interpret the decision boundaries in a decision tree?\n\nThe decision boundaries in a decision tree are represented by the splits in the tree. The splits are made by choosing a feature and a threshold value for that feature. The data points that are less than or equal to the threshold value are on one side of the boundary, and the data points that are greater than the threshold value are on the other side of the boundary.\n\n69. What is the role of feature importance in decision trees?\n\nFeature importance is a measure of how important a feature is in a decision tree. Feature importance is calculated by measuring the decrease in impurity that is caused by splitting the data on a particular feature. Feature importance can be used to interpret the model and to understand which features are most important for making predictions.\n\n70. What are ensemble techniques and how are they related to decision trees?\n\nEnsemble techniques are a way to combine multiple models to improve the accuracy of the predictions. Decision trees are often used in ensemble techniques, such as random forests and bagging.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Ensemble Techniques:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "71. What are ensemble techniques in machine learning?\n\nEnsemble techniques are a way to combine multiple models to improve the accuracy of the predictions. Ensemble techniques work by combining the predictions of multiple models to get a more accurate prediction.\n\n72. What is bagging and how is it used in ensemble learning?\n\nBagging is a type of ensemble technique that works by training multiple models on bootstrapped samples of the training data. Bootstrapped samples are samples of the training data that are drawn with replacement. This means that data points can be included in multiple bootstrapped samples.\n\n73. Explain the concept of bootstrapping in bagging.\n\nBootstrapping is a technique that is used to create multiple samples of a dataset. Bootstrapped samples are created by randomly sampling the data with replacement. This means that data points can be included in multiple bootstrapped samples.\n\n74. What is boosting and how does it work?\n\nBoosting is a type of ensemble technique that works by training multiple models sequentially. Each model is trained to correct the mistakes of the previous model. This is done by giving more weight to the data points that were misclassified by the previous model.\n\n75. What is the difference between AdaBoost and Gradient Boosting?\n\nAdaBoost and Gradient Boosting are two types of boosting algorithms. The main difference between AdaBoost and Gradient Boosting is the way that they calculate the weights for the data points. AdaBoost uses a weighted majority vote to calculate the weights, while Gradient Boosting uses a gradient descent algorithm to calculate the weights.\n\n76. What is the purpose of random forests in ensemble learning?\n\nRandom forests are a type of ensemble technique that combines bagging and decision trees. Random forests work by training multiple decision trees on bootstrapped samples of the training data. The decision trees are trained using a random subset of the features.\n\n77. How do random forests handle feature importance?\n\nRandom forests handle feature importance by calculating the Gini importance of each feature. The Gini importance of a feature is a measure of how much the feature contributes to the impurity of the decision trees in the random forest.\n\n78. What is stacking in ensemble learning and how does it work?\n\nStacking is a type of ensemble technique that works by combining the predictions of multiple models using a meta-model. The meta-model is a model that is trained to predict the predictions of the other models.\n\n79. What are the advantages and disadvantages of ensemble techniques?\n\nThe advantages of ensemble techniques include:\n\nThey can improve the accuracy of the predictions.\nThey can reduce the variance of the predictions.\nThey can be used to handle imbalanced datasets.\nThe disadvantages of ensemble techniques include:\n\nThey can be more computationally expensive than single models.\nThey can be more difficult to interpret than single models.\n80. How do you choose the optimal number of models in an ensemble?\n\nThe optimal number of models in an ensemble depends on the specific problem. However, there are a few general guidelines that can be followed. One guideline is to start with a small number of models and then increase the number of models until the accuracy of the predictions plateaus. Another guideline is to use a cross-validation technique to evaluate the performance of the ensemble with different numbers of models.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}